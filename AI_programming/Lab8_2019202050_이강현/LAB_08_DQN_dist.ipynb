{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_h7RwjJl-lG"
      },
      "source": [
        "AI Programming - SW Lee"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbJoZzXRl-lI"
      },
      "source": [
        "# Lab 08: Deep Q Learning Network (a.k.a DQN)\n",
        "## Exercise: Cart Pole, Lunar Lander"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlBL_FtHl-lI"
      },
      "source": [
        "### Prepare Library Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKN_ZFdUl-lJ",
        "outputId": "01054428-9923-425d-8da3-71c6af5a9458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.3.0)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.3.0)\n"
          ]
        }
      ],
      "source": [
        "#import library\n",
        "# Check if this code runs in Colab\n",
        "RunningInCOLAB = 'google.colab' in str(get_ipython()) # If running in colab\n",
        "\n",
        "if RunningInCOLAB:\n",
        "    !pip install swig # A library that allows code written in C to run in Python\n",
        "    !pip install gymnasium # for reinforcement learning\n",
        "    !pip install gymnasium[box2d] # for box enviroment\n",
        "    from tqdm.notebook import tqdm # to check progress\n",
        "else: # if not running in colab\n",
        "    from tqdm import tqdm # to check progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-ftx6v5El-lK"
      },
      "outputs": [],
      "source": [
        "#Import various libraries\n",
        "import os # To use functions supported by the operating system\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\" # for tensorflow environment\n",
        "\n",
        "import numpy as np # For useful array uses\n",
        "import tensorflow as tf # Deep learning library\n",
        "import keras # Deep learning library with tensorflow\n",
        "import matplotlib.pyplot as plt #for visualizing\n",
        "\n",
        "import gymnasium as gym #for reinforcement learning\n",
        "from gymnasium import wrappers #for recoding video\n",
        "\n",
        "from collections import deque # to use queue algorithm\n",
        "import random # for randomizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGbA6gG6l-lK",
        "outputId": "ede5a1a0-29dd-4653-cab1-b1306cccdda4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "#Setting GPU\n",
        "physical_devices = tf.config.list_physical_devices('GPU')# available GPU\n",
        "print(physical_devices) # print\n",
        "try:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True) #to setting GPU environment\n",
        "except:\n",
        "    print('GPU is not detected.')# cannot detect GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9QlTYJuLl-lL",
        "outputId": "e5c24fe6-e3d5-4f79-a69f-5163d9834af7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "gym.__version__# gymnasium library version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQG81nrVl-lL"
      },
      "source": [
        "### Select Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM0Q5mP7l-lL"
      },
      "source": [
        "**Cart Pole**<br>\n",
        "https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
        "\n",
        "**Lunar Lander**<br>\n",
        "https://gymnasium.farama.org/environments/box2d/lunar_lander/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DAUm7t56l-lL"
      },
      "outputs": [],
      "source": [
        "# select evironment\n",
        "# Discrete Action Space:    0 for Cartpole, 1 for LunarLander\n",
        "\n",
        "SELECT_ENV = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GMR70_Sll-lM"
      },
      "outputs": [],
      "source": [
        "#select environment\n",
        "if SELECT_ENV == 0:\n",
        "    env_name, res_prefix = 'CartPole-v1', 'cart' #set relevant environment and saved name\n",
        "    max_episodes, max_ep_steps, goal_score = 400, 500, 450 # training parameter\n",
        "    b_size, h_size = 128, 1000 # set batch_size and replay memory_size\n",
        "    network_type, state_width, state_height, state_depth = 'dense', 0, 0, 0 # these values are not use in code\n",
        "    kwargs = {'render_mode':'rgb_array'} # rendering is rgb\n",
        "elif SELECT_ENV == 1:\n",
        "    env_name, res_prefix = 'LunarLander-v3', 'lunD' #set relevant environment and saved name\n",
        "    max_episodes, max_ep_steps, goal_score = 400, 1000, 200 # training parameter\n",
        "    b_size, h_size = 128, 10000 # set batch_size and replay memory_size\n",
        "    network_type, state_width, state_height, state_depth = 'dense', 0, 0, 0 # these values are not use in code\n",
        "    kwargs = {'continuous':False, 'render_mode':'rgb_array'} # rendering is rgb\n",
        "else: assert False, 'environment selection error' #can't find environment\n",
        "\n",
        "def create_env():\n",
        "    env = gym.make(env_name, **kwargs) # create environment\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cM6_u3Oal-lM"
      },
      "outputs": [],
      "source": [
        "#define environment reset and one step\n",
        "def env_reset(env):\n",
        "    observation = env.reset()\n",
        "    state = observation[0] if type(observation)==tuple else observation\n",
        "    return state\n",
        "\n",
        "def env_step(env, action):\n",
        "    observation = env.step(action) #when one step progressed, observation have three value, state,reward,done\n",
        "    state = observation[0]\n",
        "    reward = observation[1]\n",
        "    done = observation[2] or observation[3] if len(observation)>4 else observation[2]\n",
        "    return state, reward, done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O98Iwrxfl-lM"
      },
      "outputs": [],
      "source": [
        "# environment reset and one step\n",
        "env = create_env()\n",
        "state = env_reset(env)\n",
        "state, reward, done = env_step(env, env.action_space.sample())# use sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arAcMZfYl-lM"
      },
      "source": [
        "### Check and Show Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TtOb7Aqxl-lM"
      },
      "outputs": [],
      "source": [
        "#set action_space and observation_space\n",
        "action_shape = env.action_space.shape #action_space shape\n",
        "action_space_type = type(env.action_space)\n",
        "\n",
        "if action_space_type==gym.spaces.discrete.Discrete: #we use discrete action\n",
        "    actn_space = 'DISCRETE'\n",
        "    action_shape = (1,)\n",
        "    action_dims = 1\n",
        "    action_range = env.action_space.n\n",
        "    num_actions = action_range  # number of actions is action range for DISCRETE actions\n",
        "    action_batch_shape = (None, action_range) # for replaymemory\n",
        "elif action_space_type==gym.spaces.box.Box: #don't use this space in code\n",
        "    actn_space = 'CONTINUOUS'\n",
        "    action_dims = action_shape[0]\n",
        "    actn_uppr_bound = env.action_space.high[0]\n",
        "    actn_lowr_bound = env.action_space.low[0]\n",
        "    action_range = (actn_uppr_bound - actn_lowr_bound) # x0.5 for tanh output\n",
        "    action_batch_shape = tuple([None]+[x for x in action_shape]) # for replaymemory\n",
        "    num_actions = action_dims   # number of actions is action dimension for CONTINUOUS actions\n",
        "else: assert False, 'other action space type are not supported'\n",
        "\n",
        "observation_space_type = type(env.observation_space)\n",
        "observation_shape = env.observation_space.shape #observation_space shape\n",
        "\n",
        "if observation_space_type==gym.spaces.discrete.Discrete: #don't use this space in code\n",
        "    observation_shape = (1,)\n",
        "    num_states = env.observation_space.n\n",
        "elif observation_space_type==gym.spaces.box.Box: #Cart Pole and Lunar Lander have this observation\n",
        "    num_states = observation_shape[0]\n",
        "else: print('observation space type error')\n",
        "\n",
        "state_shape = observation_shape\n",
        "state_batch_shape = tuple([None]+[x for x in observation_shape]) # for replaymemory\n",
        "\n",
        "value_shape = (1,)\n",
        "num_values = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q6eoFjHl-lM",
        "outputId": "3ef1c75c-a719-475e-a0d3-5d26ff967f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action space  <class 'gymnasium.spaces.discrete.Discrete'>\n",
            "Action shape  (1,)\n",
            "Action dimensions  1\n",
            "Action range  2\n",
            "Action batch shape  (None, 2)\n",
            "Observation space  <class 'gymnasium.spaces.box.Box'>\n",
            "Observation shape  (4,)\n",
            "Size of State Space  4\n",
            "State shape  (4,)\n",
            "State batch shape  (None, 4)\n",
            "Vallue shape  (1,)\n",
            "Value dimensions  1\n"
          ]
        }
      ],
      "source": [
        "#check action,observation,state,value\n",
        "print('Action space ', action_space_type)\n",
        "print('Action shape ', action_shape)\n",
        "print('Action dimensions ', action_dims)\n",
        "print('Action range ', action_range)\n",
        "if action_space_type==gym.spaces.box.Box:\n",
        "    print('Max Value of Action ', actn_uppr_bound)\n",
        "    print('Min Value of Action ', actn_lowr_bound)\n",
        "else: pass\n",
        "print('Action batch shape ', action_batch_shape)\n",
        "\n",
        "print('Observation space ', observation_space_type)\n",
        "print('Observation shape ', observation_shape)\n",
        "print('Size of State Space ', num_states)\n",
        "print('State shape ', state_shape)\n",
        "print('State batch shape ', state_batch_shape)\n",
        "\n",
        "print('Vallue shape ', value_shape)\n",
        "print('Value dimensions ', num_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlLaqp4yl-lN"
      },
      "source": [
        "### Define and Initialize The Agent\n",
        "### **Exercise:** Define Deep Q-network (TensorFlow)\n",
        "\n",
        "A NN of three fully-connected layers is enough for classic control problems.<br>\n",
        "\n",
        "**Parameters for layer definition are:**<br>\n",
        "hiddens = (unit # for layer1, unit # for layer2),<br>\n",
        "act_fn: activation function,<br>\n",
        "out_fn: activation function for output layer, <br>\n",
        "init_fn: kernel initialization function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ilnrayiBl-lN"
      },
      "outputs": [],
      "source": [
        "#define DQNet function\n",
        "def DQNet(hiddens, act_fn, out_fn, init_fn):    # hiddends = (layer1 units, layer2 units)\n",
        "    inputs = keras.Input(shape=state_shape)  # input layer\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    l1 = tf.keras.layers.Dense(units=hiddens[0], activation=act_fn, kernel_initializer=init_fn)(inputs) # first fully connected layer\n",
        "                                                                                                        #  (units=, activation=, kernel_initializer=)\n",
        "    l2 = tf.keras.layers.Dense(units=hiddens[1], activation=act_fn, kernel_initializer=init_fn)(l1) # second fully connected layer\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=num_actions, activation=out_fn, kernel_initializer=init_fn)(l2) # output (third) layer\n",
        "\n",
        "    ###  END CODE HERE  ###\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name='q_net')\n",
        "    return model\n",
        "\n",
        "def build_DQNet():\n",
        "    model = DQNet(hiddens=(32,32), act_fn='relu', out_fn='linear', init_fn='he_uniform')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "P6fMhjGHl-lN",
        "outputId": "4895d72e-6a9d-4a5c-a866-6c85d16ee59c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"q_net\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"q_net\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m160\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m1,056\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │              \u001b[38;5;34m66\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,282\u001b[0m (5.01 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,282</span> (5.01 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,282\u001b[0m (5.01 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,282</span> (5.01 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#test model and check inside\n",
        "test_model = build_DQNet()\n",
        "test_model.summary()\n",
        "del test_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSCoX_MVl-lN"
      },
      "source": [
        "**Model Summary:**\n",
        "\n",
        "```\n",
        "┏--------------------------------------┳-----------------------------┳-----------------┓\n",
        "┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n",
        "┡--------------------------------------╇-----------------------------╇-----------------┩\n",
        "│ input_layer (InputLayer)             │ (None, 4 or 8)              │               0 │\n",
        "├--------------------------------------┼-----------------------------┼-----------------┤\n",
        "│ dense (Dense)                        │ (None, 32)                  │      160 or 288 │\n",
        "├--------------------------------------┼-----------------------------┼-----------------┤\n",
        "│ dense_1 (Dense)                      │ (None, 32)                  │           1,056 │\n",
        "├--------------------------------------┼-----------------------------┼-----------------┤\n",
        "│ dense_2 (Dense)                      │ (None, 2 or 4)              │       66 or 132 │\n",
        "└--------------------------------------┴-----------------------------┴-----------------┘\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBJuWcDXl-lN"
      },
      "source": [
        "The policy function in the Agent_Net is just for getting a SINGLE sample, not for training. So you have to set `training=False` if necessary.\n",
        "Since the network takes batch format (i.e., `[batch,data]`), the input state should be given as `[None,...]` and the output should be taken as `[0,...]`.\n",
        "\n",
        "### **Exercise:** Define Policy Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e9iB5838l-lO"
      },
      "outputs": [],
      "source": [
        "#agent(Q network, target network)\n",
        "class Agent_Net:\n",
        "    def __init__(self):\n",
        "        self.policy_q = build_DQNet()                       # build policy network\n",
        "        self.target_q = build_DQNet()                       # build target network\n",
        "        self.target_update()                                # copy weights from policy to target\n",
        "\n",
        "    def policy(self, state, epsilon, exploring):            # e-greedy policy if exploring==True\n",
        "        state_input = tf.convert_to_tensor(state[None,...], dtype=tf.float32)   # make the state network-ready\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        if exploring:                                       # if e-greedy policy\n",
        "            if tf.random.uniform(()) > epsilon:                         #  exploit if random>epsilon\n",
        "                action_q = self.policy_q(state_input)                                 #   get actions for input state\n",
        "                action = tf.math.argmax(action_q[0])                                   #   find an action for maximum q value\n",
        "            else:                                           #  explore else\n",
        "                action = tf.random.uniform((), minval=0, maxval=action_range, dtype=tf.int64)  #   random action\n",
        "        else:                                               # else greedy policy (exploitation)\n",
        "            action_q = self.policy_q(state_input)                                     #  get actions for input state\n",
        "            action = tf.math.argmax(action_q[0])                                       #  find an action for maximum q value\n",
        "\n",
        "        ###  END CODE HERE  ###\n",
        "\n",
        "        return action.numpy()\n",
        "\n",
        "    def target_update(self):\n",
        "        self.target_q.set_weights(self.policy_q.get_weights())  # copy weights from policy network to target network\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPbCXorIl-lO",
        "outputId": "e8c648c9-d981-4c52-ee39-1837d4a143da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5  0  6  5  1  3  1  4  5  0  3  1  6  0  1  2  0  3  6  4  "
          ]
        }
      ],
      "source": [
        "# Check whether the implemented code works well\n",
        "def test_policy(exploring):\n",
        "\n",
        "    epsilon = 0.1\n",
        "    state_input = tf.random.uniform((1,3)) # 1x3 state input\n",
        "    action_range = 7\n",
        "\n",
        "    def policy_q(x):    # pretending policy_q network\n",
        "        x = tf.random.uniform((tf.shape(x)[0],action_range))\n",
        "        return x\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    if exploring:                                                                       # if e-greedy policy\n",
        "        if tf.random.uniform(()) > epsilon:                                             #  exploit if random>epsilon\n",
        "            action_q = policy_q(state_input)                                            #   get actions for input state\n",
        "            action = tf.math.argmax(action_q[0])                                        #   find an action for maximum q value\n",
        "        else:                                                                           #  explore else\n",
        "            action = tf.random.uniform((),minval=0,maxval=action_range,dtype=tf.int64)  #   random action\n",
        "    else:                                                                               # else greedy policy (exploitation)\n",
        "        action_q = policy_q(state_input)                                                #  get actions for input state\n",
        "        action = tf.math.argmax(action_q[0])                                            #  find an action for maximum q value\n",
        "\n",
        "    ###  END CODE HERE  ###\n",
        "\n",
        "    return action\n",
        "\n",
        "tf.random.set_seed(2) # Use consistent random values\n",
        "for _ in range(10): print(test_policy(True).numpy(), ' ', end='')\n",
        "for _ in range(10): print(test_policy(False).numpy(), ' ', end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plYK_QZPl-lO"
      },
      "source": [
        "**Expected Outputs:**\n",
        "\n",
        "```\n",
        "5  0  6  5  1  3  1  4  5  0  3  1  6  0  1  2  0  3  6  4\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrG-ULlyl-lO"
      },
      "source": [
        "### Define and Initialize Replay Memory\n",
        "\n",
        "The replay memory (or replay buffer) is implemented with `deque`, which maintians the fixed number of elements by discarding the oldest element automatically.<br>\n",
        "The inputs of the `put_experience` function are from environments and the outputs of the `get_batch` function are fed to the NN. Therefore the output types should be tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OJS1YOExl-lO"
      },
      "outputs": [],
      "source": [
        "#define replay memory\n",
        "class ReplayMemory:\n",
        "    def __init__(self, memory_size):\n",
        "        self.experiences = deque(maxlen=memory_size)    # allocate replay memory\n",
        "        self.num_episodes = 0                           # set number of episode to zero\n",
        "\n",
        "    def put_experience(self, experience):               # put an experience into replay memory\n",
        "        state, action, next_state, reward, not_done = experience\n",
        "        self.experiences.append((state, action, next_state, reward, not_done)) #bundle and save\n",
        "        return\n",
        "\n",
        "    def get_batch(self, num_samples):                   # get a batch of randomly sampled experiences\n",
        "        state_batch, next_state_batch, action_batch, reward_batch, not_done_batch = [], [], [], [], []\n",
        "\n",
        "        sample_batch = random.sample(self.experiences, num_samples) # shuffle\n",
        "\n",
        "        for sample in sample_batch: # Take each item out of the sample and save it to the list\n",
        "            state, action, next_state, reward, not_done = sample\n",
        "            state_batch.append(state)\n",
        "            action_batch.append(action)\n",
        "            next_state_batch.append(next_state)\n",
        "            reward_batch.append(reward)\n",
        "            not_done_batch.append(not_done)\n",
        "\n",
        "        batch = (tf.convert_to_tensor(state_batch, dtype=tf.float32),\n",
        "                tf.convert_to_tensor(action_batch, dtype=tf.int32),\n",
        "                tf.convert_to_tensor(next_state_batch, dtype=tf.float32),\n",
        "                tf.convert_to_tensor(reward_batch, dtype=tf.float32),\n",
        "                tf.convert_to_tensor(not_done_batch, dtype=tf.float32))\n",
        "        return batch #return tensor type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZETJeaMl-lO"
      },
      "source": [
        "### **Exercise:** Initialize the replay memory\n",
        "\n",
        "A single experience consists of `(state, action, next_state, reward, not_done)`. Be careful with **`not_done`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ntMtcOval-lO"
      },
      "outputs": [],
      "source": [
        "#initialize replay memory\n",
        "def init_memory(mem, env, agent, num_samples):\n",
        "    state = env_reset(env)                          # initialize environment\n",
        "    for _ in range(num_samples):\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        action = agent.policy(state, epsilon=1.0, exploring=True) # get an action with the policy\n",
        "        next_state, reward, done = env_step(env, action)                 # observe the environment reaction\n",
        "        experience = (state, action, next_state, reward, not done)       # pack observations into experience tuple, done is converted to not done\n",
        "        mem.put_experience(experience)                       # put the experience to replay memory\n",
        "        state = env_reset(env) if done else next_state # set the next state (reset env if done)\n",
        "\n",
        "        ###  END CODE HERE  ###\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkywtMPul-lP",
        "outputId": "5c3107b9-2bc1-41e0-e15b-eda7dbb52488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.04072088  0.18846463 -0.00277539 -0.32736924]\n",
            "[1 0 1 0]\n",
            "[-0.02550636 -0.00617038 -0.02836518 -0.04544564]\n",
            "[1. 1. 1. 1.]\n",
            "[1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "#test replay memory\n",
        "tf.random.set_seed(2) #extract random value\n",
        "keras.utils.set_random_seed(2)\n",
        "test_agent = Agent_Net() #target network\n",
        "test_mem = ReplayMemory(4) #replay memory\n",
        "test_env = create_env() #environment\n",
        "test_state = test_env.reset(seed=3) #state\n",
        "init_memory(test_mem, test_env, test_agent, 4) #initialize memory\n",
        "# There is no difference between if statement and else statement.\n",
        "# When the value is 1, the state has 8 dimensions, so extracting 8 seems to be meaningful as a branch.\n",
        "if SELECT_ENV==1:\n",
        "    print(test_mem.get_batch(4)[0][0][:4].numpy())\n",
        "    print(test_mem.get_batch(4)[1].numpy())\n",
        "    print(test_mem.get_batch(4)[2][0][:4].numpy())\n",
        "    print(test_mem.get_batch(4)[3].numpy())\n",
        "    print(test_mem.get_batch(4)[4].numpy())\n",
        "else:\n",
        "    print(test_mem.get_batch(4)[0][0][:4].numpy())\n",
        "    print(test_mem.get_batch(4)[1].numpy())\n",
        "    print(test_mem.get_batch(4)[2][0][:4].numpy())\n",
        "    print(test_mem.get_batch(4)[3].numpy())\n",
        "    print(test_mem.get_batch(4)[4].numpy())\n",
        "\n",
        "# finishing work\n",
        "del test_agent, test_mem\n",
        "test_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1KgVFBOl-lP"
      },
      "source": [
        "**Expected Outputs:**\n",
        "\n",
        "```\n",
        "[-0.04072088  0.18846463 -0.00277539 -0.32736924]\n",
        "[1 0 1 0]\n",
        "[-0.02550636 -0.00617038 -0.02836518 -0.04544564]\n",
        "[1. 1. 1. 1.]\n",
        "[1. 1. 1. 1.]\n",
        "```\n",
        "or\n",
        "```\n",
        "[-0.01505842  1.4191642  -0.76619244  0.17035668]\n",
        "[1 0 1 0]\n",
        "[-0.03796177  1.427054   -0.7755038   0.08992036]\n",
        "[-1.0618883 -1.0290995 -1.9180926 -1.4797792]\n",
        "[1. 1. 1. 1.]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCNjRnBZl-lP"
      },
      "source": [
        "### Learning Procedures\n",
        "\n",
        "DQN trains the Q-network to minimize the difference:\n",
        "\n",
        "$$ \\delta_t = (r_{t} + \\gamma \\max_{\\hat{a}_{t+1}} Q_T(s_{t+1}, \\hat{a}_{t+1}; \\theta_T)) - Q_P(s_t, a_t; \\theta_P) $$\n",
        "\n",
        "where $Q_T$ is a target network and $Q_P$ policy network.\n",
        "\n",
        "Then the loss function is\n",
        "\n",
        "$$ L(\\theta_P) = \\delta_t^2 $$\n",
        "\n",
        "### **Exercise:** Define ONE step of Training Loop and Evaluation Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alqq6yWtl-lP"
      },
      "source": [
        "The following test has same codes as the `dqn_train` function except the last line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MKfz6UUml-lP"
      },
      "outputs": [],
      "source": [
        "#for test\n",
        "def test_training():\n",
        "    state_b = tf.random.uniform((3,4))\n",
        "    action_b = tf.random.uniform((3,), minval=0, maxval=7, dtype=tf.int32)\n",
        "    next_state_b = tf.random.uniform((3,4))\n",
        "    reward_b = tf.random.uniform((3,))\n",
        "    not_done_b = tf.random.uniform((3,))\n",
        "    gamma = tf.random.uniform((1,))\n",
        "    action_range = 7\n",
        "\n",
        "    class test_agent:\n",
        "        def __init__(self):\n",
        "            pass\n",
        "        def policy_q(self, x):\n",
        "            return tf.reduce_sum(x, axis=-1, keepdims=True) # The way to obtain the q value through the state is through a Q network, but since this is a test, it is implemented as a simple sum.\n",
        "        def target_q(self, x):\n",
        "            return tf.reduce_sum(x, axis=-1, keepdims=True) # The way to obtain the q value through the state is through a target network, but since this is a test, it is implemented as a simple sum.\n",
        "\n",
        "    agent = test_agent()\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # get action probability with current (think of WHY!!) policy (b,a)\n",
        "    curr_q = agent.policy_q(state_b) #calculate Q value\n",
        "\n",
        "    # get action probability with target policy (b,a),\n",
        "    # and then find the max Q value with it (b,)\n",
        "    next_q = agent.target_q(next_state_b) #calculate Q value\n",
        "    max_next_q = tf.reduce_max(next_q,axis=-1) # remove axis\n",
        "\n",
        "    # calculate target reward (b,1)\n",
        "    target_reward = reward_b + gamma * max_next_q * not_done_b\n",
        "\n",
        "    # make one-hot actions (b,a) to filter out other actions\n",
        "    action_v = tf.one_hot(action_b,depth=action_range)\n",
        "\n",
        "    # make ground true labels for training (b,a)\n",
        "    label_q = curr_q + (tf.expand_dims(target_reward,-1) - curr_q) * action_v\n",
        "    ###  END CODE HERE  ###\n",
        "\n",
        "    return label_q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo347c8Zl-lP",
        "outputId": "e483a7bf-8245-4eeb-96df-75fe72b4f56b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training lable test passed.\n"
          ]
        }
      ],
      "source": [
        "# Check if q-learning is working well\n",
        "tf.random.set_seed(2)\n",
        "\n",
        "ans = np.array([[1.1889474 , 1.1889474 , 1.1889474 , 1.1889474 , 1.1889474 , 1.1889474 , 0.40853113],\n",
        "                [2.713482  , 2.713482  , 2.4351463 , 2.713482  , 2.713482  , 2.713482  , 2.713482  ],\n",
        "                [3.3669732 , 1.4427134 , 3.3669732 , 3.3669732 , 3.3669732 , 3.3669732 , 3.3669732 ]])\n",
        "res = test_training()\n",
        "print('Training lable test passed.') if np.allclose(res,ans) else print('Training lable test failed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "N5-WhBJFl-lQ"
      },
      "outputs": [],
      "source": [
        "# train network\n",
        "def dqn_train(agent, batch, config):\n",
        "    state_b, action_b, next_state_b, reward_b, not_done_b = batch #load from batch\n",
        "    gamma = config.gamma\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # get action probability with current (think of WHY!!) policy (b,a)\n",
        "    curr_q = agent.policy_q(state_b)\n",
        "\n",
        "    # get action probability with target policy (b,a),\n",
        "    # and then find the max Q value with it (b,)\n",
        "    next_q = agent.target_q(next_state_b)\n",
        "    max_next_q = tf.reduce_max(next_q,axis=-1)\n",
        "\n",
        "    # calculate target reward (b,1)\n",
        "    target_reward = reward_b + gamma * max_next_q * not_done_b\n",
        "\n",
        "    # make one-hot actions (b,a) to filter out other actions\n",
        "    action_v = tf.one_hot(action_b,depth=action_range)\n",
        "\n",
        "    # make ground true labels for training (b,a)\n",
        "    label_q = curr_q + (tf.expand_dims(target_reward,-1) - curr_q) * action_v\n",
        "\n",
        "    # training with model.fit()\n",
        "    logs = agent.policy_q.fit(state_b, label_q, epochs=1, verbose=0)\n",
        "\n",
        "    ###  END CODE HERE  ###\n",
        "\n",
        "    loss = logs.history['loss'][-1]\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "04q2HAxFl-lQ"
      },
      "outputs": [],
      "source": [
        "# Evaluation after training\n",
        "def evaluate_policy(env, agent, num_avg):\n",
        "\n",
        "    total_reward = 0.0\n",
        "    episodes_to_play = num_avg\n",
        "    for i in range(episodes_to_play): # Play n episode and take the average\n",
        "        state = env_reset(env)\n",
        "        done = False\n",
        "        episode_reward = 0.0\n",
        "        while not done:\n",
        "\n",
        "            ### START CODE HERE ###\n",
        "\n",
        "            action = agent.policy(state,0,False)                              # get an action with policy\n",
        "            next_state, reward, done = env_step(env,action)             # take action and observe outcomes\n",
        "\n",
        "            ###  END CODE HERE  ###\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "        total_reward += episode_reward\n",
        "    average_reward = total_reward / episodes_to_play\n",
        "\n",
        "    return average_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnDWJC-zl-lQ"
      },
      "source": [
        "### Define Epsilon Function\n",
        "\n",
        "This is an example of exponential decay epsilon function. One of easist epsilon decay functions is simply to multiply 0.9. You can define your own epsilon function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "72GTpL2Rl-lQ"
      },
      "outputs": [],
      "source": [
        "# Exploration parameters for epsilon greedy strategy\n",
        "class Epsilon:\n",
        "    def __init__(self, max_episodes, decay_speed=1.0):\n",
        "        self.explore_start = 1.0            # exploration probability at start\n",
        "        self.explore_stop = 0.01            # minimum exploration probability\n",
        "        self.decay_rate = decay_speed/max_episodes  # exp decay rate for exploration prob (10/max ≈ 0.99)\n",
        "        self.episode_cnt = 0\n",
        "\n",
        "    def get_epsilon(self):\n",
        "        eps = (self.explore_stop\n",
        "            + (self.explore_start - self.explore_stop) * tf.math.exp(-self.decay_rate * self.episode_cnt)) # Decrease epsilon gradually (early exploratory, late follow learned)\n",
        "        self.episode_cnt += 1\n",
        "        return eps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT5Kprvcl-lQ"
      },
      "source": [
        "### Define and Initialize Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XcZ_DUkCl-lU"
      },
      "outputs": [],
      "source": [
        "#set hyperparameter\n",
        "class configuration:\n",
        "    def __init__(self):\n",
        "        self.gamma = 0.99   # discount rate\n",
        "        self.lr = 2e-4      # learning rate\n",
        "\n",
        "config = configuration()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Mp1Bnz_ul-lU"
      },
      "outputs": [],
      "source": [
        "#Replay memory size setting, batch size setting, related optimizer setting\n",
        "max_steps = max_episodes * max_ep_steps\n",
        "batch_size = b_size\n",
        "memory_size = h_size\n",
        "\n",
        "agent = Agent_Net()\n",
        "\n",
        "memD = ReplayMemory(memory_size)\n",
        "init_memory(memD, env, agent, memory_size) #replay memory init\n",
        "epsF = Epsilon(max_episodes, 10.0) #epsilon setting\n",
        "opt = tf.optimizers.Adam(learning_rate=config.lr, clipvalue=2.0)\n",
        "\n",
        "agent.policy_q.compile(optimizer=opt, loss='mse', jit_compile=False) #precompile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDDATHlxl-lV"
      },
      "source": [
        "### Define Main Training Loop\n",
        "\n",
        "\n",
        "### **Exercise:** Complete Main Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TQOZzxjcl-lV",
        "outputId": "9edea53a-eb53-49bf-aa97-eff655922388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405,
          "referenced_widgets": [
            "90de667b645748b1b9e6ad874a3d0cc8",
            "55e31e2e892340cab81df606aac6ee1d",
            "599cad77f0234bf88aa6b60c7700da1e",
            "d7ea0f73b3b84086a3cc48ae6b83520c",
            "b0c28f65b1de423d9a70d086188a9ad9",
            "9355f6f675404a47bd89e238c0444a59",
            "c05035dd411945efa770d30f1a0c7c16",
            "dc1e2857e9154b62982e6b93569f298e",
            "03291b2d0d394e06853b3f7e3b6f470a",
            "34733efa1d7d427cb9afc19b339a3d7f",
            "aaa38a4c0b864e0fbd4b4ac53d6bb9f9"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/200000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90de667b645748b1b9e6ad874a3d0cc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-f411c20b33fe>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m                                    \u001b[0;31m# get a new batch from replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mstep_pi_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m                        \u001b[0;31m# train DQN for a step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m###  END CODE HERE  ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-29d1b1b11c0f>\u001b[0m in \u001b[0;36mdqn_train\u001b[0;34m(agent, batch, config)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# training with model.fit()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m###  END CODE HERE  ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#final main train code\n",
        "#log\n",
        "logs = keras.callbacks.History()\n",
        "logs.history.update({'pi_loss':[]})\n",
        "logs.history.update({'ereward':[]})\n",
        "logs.history.update({'e-steps':[]})\n",
        "logs.history.update({'vreward':[]})\n",
        "\n",
        "# variables for simulation\n",
        "num_episodes = 0\n",
        "val_episodes = 2            # exit condition\n",
        "\n",
        "# variables for episode logging\n",
        "pi_loss = 0.0\n",
        "#initial reward and loss value\n",
        "loss_sum = 0.0\n",
        "epis_steps = 0\n",
        "epis_reward = 0.0\n",
        "eval_reward = -float('inf')\n",
        "\n",
        "# initialize training variables\n",
        "epsilon = 1.0\n",
        "next_state = None\n",
        "done = True\n",
        "\n",
        "pbar = tqdm(range(max_steps), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}') #progress bar\n",
        "\n",
        "for sim_steps in pbar:\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    state = env_reset(env) if done else next_state                        # get the current state\n",
        "    action = agent.policy(state,epsilon,exploring=True)                   # find an action with e-greedy\n",
        "    next_state, reward, done = env_step(env, action)                      # take action and observe outcomes\n",
        "\n",
        "    experience = (state, action, next_state, reward, not done)            # pack observations into a new experience\n",
        "    memD.put_experience(experience)                                       # put a new experience to replay buffer\n",
        "\n",
        "    batch = memD.get_batch(batch_size)                                    # get a new batch from replay buffer\n",
        "    step_pi_loss = dqn_train(agent, batch, config)                        # train DQN for a step\n",
        "\n",
        "    ###  END CODE HERE  ###\n",
        "\n",
        "    loss_sum += step_pi_loss                                # accumulate policy loss for a step\n",
        "    epis_reward += reward                                   # accumulate reward for a step\n",
        "    epis_steps += 1                                         # increase the number of steps for an episode\n",
        "\n",
        "    # episode termination conditions\n",
        "    if epis_steps>max_ep_steps: done = True\n",
        "\n",
        "    # summarize episode\n",
        "    if done:\n",
        "        agent.target_update()                               # update target network whenever episode ends\n",
        "        memD.num_episodes += 1                              # increase number of episode simulated\n",
        "        epsilon = epsF.get_epsilon()                        # update decay epsilon value\n",
        "\n",
        "        pi_loss = loss_sum / epis_steps                     # average policy loss for an episode\n",
        "\n",
        "        pbar.set_postfix({'episode':num_episodes, 'loss':step_pi_loss, 'reward':eval_reward, 'steps':epis_steps, 'evaluating':val_episodes})\n",
        "        eval_reward = evaluate_policy(env, agent, 1)        # evaluate policy one time\n",
        "\n",
        "        #log append\n",
        "        logs.history['pi_loss'].extend([pi_loss])\n",
        "        logs.history['ereward'].extend([epis_reward])\n",
        "        logs.history['e-steps'].extend([epis_steps])\n",
        "        logs.history['vreward'].extend([eval_reward])\n",
        "\n",
        "        loss_sum = 0.0\n",
        "        epis_reward = 0.0\n",
        "        epis_steps = 0\n",
        "        num_episodes += 1\n",
        "\n",
        "    else: pass\n",
        "\n",
        "    pbar.set_postfix({'episode':num_episodes, 'loss':step_pi_loss, 'reward':eval_reward, 'steps':epis_steps})\n",
        "\n",
        "    # coditions to stop simulation\n",
        "    if eval_reward > goal_score:\n",
        "        eval_reward = evaluate_policy(env, agent, val_episodes) # evaluate policy multiple times\n",
        "    if eval_reward > goal_score: break\n",
        "    if num_episodes > max_episodes: break\n",
        "\n",
        "print('episodes:{0:5d}, loss:{1:7.5f}, val_reward {2:4.2f}'.format(num_episodes, pi_loss, eval_reward))\n",
        "print('total steps:', sim_steps+1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ7Dnduol-lV"
      },
      "source": [
        "### Plot Training Histories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2Dl3ZL8l-lV"
      },
      "outputs": [],
      "source": [
        "# plot loss and accuracy\n",
        "def plot_graphs(log_history, log_labels, graph_labels, graph_colors=['b-','g-']):\n",
        "    num_graphs = len(log_labels)\n",
        "    plt.figure(figsize=(5*num_graphs,4))\n",
        "    for i in range(num_graphs):\n",
        "        plt.subplot(1,num_graphs,i+1)\n",
        "        plt.plot(log_history[log_labels[i]], graph_colors[i], label=graph_labels[i])\n",
        "        plt.xlabel('episodes')\n",
        "        plt.legend()\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "log_labels    = ['pi_loss', 'vreward']# Correlation between loss and reward\n",
        "label_strings = ['loss', 'reward']\n",
        "label_colors  = ['b-', 'g-'] # blue and green\n",
        "plot_graphs(logs.history, log_labels, label_strings, label_colors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2iyXnxGl-lW"
      },
      "source": [
        "### Evaluate the Agent\n",
        "\n",
        "Since a single evaluation try often takes some time, evaluate the agent here to show the progress bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt1JGpNSl-lW"
      },
      "outputs": [],
      "source": [
        "#evaluate agent\n",
        "evaluate_episodes = 20\n",
        "sum_episode_rewards = 0.0\n",
        "pbar = tqdm(range(evaluate_episodes))\n",
        "\n",
        "for i in pbar:\n",
        "    sum_episode_rewards += evaluate_policy(env, agent, 1) #Evaluate 20 episodes once each\n",
        "\n",
        "env.close()\n",
        "\n",
        "print('Evaluation Result:',  sum_episode_rewards/evaluate_episodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjpQXOqIl-lW"
      },
      "source": [
        "## Show How The Agent Works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qdhzihJl-lW"
      },
      "outputs": [],
      "source": [
        "#save video\n",
        "env = create_env()\n",
        "env = wrappers.RecordVideo(env, video_folder='./gym-results/', name_prefix=res_prefix)\n",
        "\n",
        "eval_reward = evaluate_policy(env, agent, 1) #evaluate 1\n",
        "\n",
        "print('Sample Total Reward:', eval_reward)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJKlT700l-lW"
      },
      "outputs": [],
      "source": [
        "#show video\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "def show_video(video_path, video_width = 320):\n",
        "  video_file = open(video_path, \"r+b\").read() #load saved video\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\" #encode,decode\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")#put link\n",
        "\n",
        "show_video('./gym-results/' + res_prefix + '-episode-0.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEceFz57l-lW"
      },
      "source": [
        "(c) 2024 SW Lee"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "71930d9c743a2c2f7d41567bb1e631f1d30be1b0f7ff3429fb86acce8edbed56"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "90de667b645748b1b9e6ad874a3d0cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55e31e2e892340cab81df606aac6ee1d",
              "IPY_MODEL_599cad77f0234bf88aa6b60c7700da1e",
              "IPY_MODEL_d7ea0f73b3b84086a3cc48ae6b83520c"
            ],
            "layout": "IPY_MODEL_b0c28f65b1de423d9a70d086188a9ad9"
          }
        },
        "55e31e2e892340cab81df606aac6ee1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9355f6f675404a47bd89e238c0444a59",
            "placeholder": "​",
            "style": "IPY_MODEL_c05035dd411945efa770d30f1a0c7c16",
            "value": ""
          }
        },
        "599cad77f0234bf88aa6b60c7700da1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc1e2857e9154b62982e6b93569f298e",
            "max": 200000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03291b2d0d394e06853b3f7e3b6f470a",
            "value": 433
          }
        },
        "d7ea0f73b3b84086a3cc48ae6b83520c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34733efa1d7d427cb9afc19b339a3d7f",
            "placeholder": "​",
            "style": "IPY_MODEL_aaa38a4c0b864e0fbd4b4ac53d6bb9f9",
            "value": "  0%|          | 433/200000 [00:46&lt;4:59:25, 11.11it/s, episode=29, loss=38.1, reward=9, steps=0]"
          }
        },
        "b0c28f65b1de423d9a70d086188a9ad9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9355f6f675404a47bd89e238c0444a59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c05035dd411945efa770d30f1a0c7c16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc1e2857e9154b62982e6b93569f298e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03291b2d0d394e06853b3f7e3b6f470a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34733efa1d7d427cb9afc19b339a3d7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaa38a4c0b864e0fbd4b4ac53d6bb9f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}