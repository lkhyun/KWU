{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Programming - SW Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Optimizers for Deep Neural Networks\n",
    "## Exercise: Predicting MNIST Digits\n",
    "### For this exercise, prepare privious Lab to copy your previous implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhC9a_u5Ta4u"
   },
   "source": [
    "### Prepare Mini-MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MboBxtwcTa41"
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "# digits.data from sklearn contains 1797 images of 8x8 pixels\n",
    "# Each image has a hand-written digit\n",
    "digits_df = digits.images.reshape((len(digits.target), -1))\n",
    "digits_tf = digits.target\n",
    "\n",
    "# Splitting dataframe into train & test\n",
    "X_train_org, X_test_org, y_train_num, y_test = train_test_split(digits_df, digits_tf, test_size= 0.20, random_state= 101)\n",
    "\n",
    "# Digits data has range of [0,16], which often lead too big exponential values\n",
    "# so make them normal distribution of [0,1] with the sklearn package, or you can just divide them by 16\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train_org)\n",
    "X_test = sc.transform(X_test_org)\n",
    "\n",
    "n_classes = 10\n",
    "\n",
    "# Transform Nx1 Y vector into Nx10 answer vector, so that we can perform one-to-all classification\n",
    "y_train = np.zeros((y_train_num.shape[0],10))\n",
    "for i in range(n_classes):\n",
    "    y_train[:,i] = (y_train_num == i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.math import sigmoid as tf_sigmoid\n",
    "from tensorflow.nn import softmax as tf_softmax\n",
    "\n",
    "def sigmoid(x):\n",
    "    x = tf_sigmoid(x)\n",
    "    return x.numpy()\n",
    "\n",
    "def softmax(x):\n",
    "    x = tf_softmax(x)\n",
    "    return x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1437, 64)\n",
      "(1437, 10)\n",
      "[ 0.  0.  0.  9. 16.  6.  0.  0.  0.  0.  4. 15.  6. 15.  0.  0.  0.  0.\n",
      "  8. 11.  9. 11.  0.  0.  0.  0.  8. 16. 14.  2.  0.  0.  0.  0. 11. 16.\n",
      " 13.  0.  0.  0.  0.  6. 14.  2. 12.  9.  0.  0.  0.  5. 16. 11.  5. 13.\n",
      "  4.  0.  0.  0.  3.  8. 13. 16.  9.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number is 6\n"
     ]
    }
   ],
   "source": [
    "print(digits_df.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_train_org[0])\n",
    "\n",
    "idx = np.random.randint(X_train.shape[0])\n",
    "dimage = X_train_org[idx].reshape((8,8))\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.gray()\n",
    "plt.matshow(dimage, fignum=1)\n",
    "plt.show()\n",
    "print('The number is', y_train_num[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple DNN for Digit Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDenseLayer:\n",
    "    def __init__(self, n_out, n_in):\n",
    "        self.wegt = np.empty((n_out, n_in))\n",
    "        self.bias = np.zeros((n_out))\n",
    "        self.saved_x = None     # store x to use while backpropagation\n",
    "\n",
    "    def forward(self, x):       # (b, i)\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        self.saved_x = x     # keep it for backward\n",
    "        x_lin = (self.wegt @ x.T).T + self.bias            # Linear Prediction\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        return x_lin\n",
    "\n",
    "    def backward(self, x, x_in):  # x = dJ/dz (b, c)\n",
    "        assert np.array_equal(self.saved_x, x_in), print('x_in does not equal to input X.')\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        dw = x.T @ x_in               # Gradients for weights\n",
    "        db = np.sum(x,axis=0)               # Gradients for biases\n",
    "        wdJdz = x @ self.wegt            # Propagation for lower layer\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        return dw, db, wdJdz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Backpropagation of Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJdz_sigmoid(wdJdz_upper, az):\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    dJdz = wdJdz_upper * az * (1-az)            # backpropagation through activation function\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return dJdz\n",
    "\n",
    "def dJdz_softmax(y_hat, y):\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    dJdz = y_hat - y            # backpropagation through activation function\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return dJdz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_forward(layers, X_in):\n",
    "    l1, l2, l3 = layers\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    a_1 = sigmoid(l1.forward(X_in))                    # first stage forward\n",
    "    a_2 = sigmoid(l2.forward(a_1))                    # second stage forward\n",
    "    a_3 = softmax(l3.forward(a_2))                    # third stage forward\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return a_1, a_2, a_3\n",
    "\n",
    "def my_backward(layers, a_1, a_2, a_3, X_in, y_true):\n",
    "    l1, l2, l3 = layers\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    dw_3, db_3, wdJdz_3 = l3.backward(dJdz_softmax(a_3,y_true),a_2)    # go through 3rd stage backward\n",
    "    dw_2, db_2, wdJdz_2 = l2.backward(dJdz_sigmoid(wdJdz_3,a_2),a_1)    # go through 2nd stage backward\n",
    "    dw_1, db_1, _       = l1.backward(dJdz_sigmoid(wdJdz_2,a_1),X_in)    # go through 1st stage backward\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    d_1 = [dw_1, db_1]\n",
    "    d_2 = [dw_2, db_2]\n",
    "    d_3 = [dw_3, db_3]\n",
    "    return d_1, d_2, d_3\n",
    "\n",
    "def my_loss(layers, X_in, y_true):\n",
    "    ### START CODE HERE ###\n",
    "    _,_,a_3 = my_forward(layers,X_in)\n",
    "    loss = np.sum(-y_true*np.log(a_3)) / X_in.shape[0]                    # calculate loss\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return loss\n",
    "    \n",
    "def my_predict(layers, X_in):\n",
    "    ### START CODE HERE ###\n",
    "    _,_,a_3 = my_forward(layers,X_in)\n",
    "    pred = np.argmax(a_3,axis = 1)                   # make prediction\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a NN model and check the matrix dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DT0rMw-rTa5A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64) (1437, 10)\n",
      "(80, 64) (80,)\n",
      "(70, 80) (70,)\n",
      "(10, 70) (10,)\n"
     ]
    }
   ],
   "source": [
    "n_inputs  = 64\n",
    "n_hidden1 = 80\n",
    "n_hidden2 = 70\n",
    "n_classes = 10\n",
    "#Define parameters\n",
    "l1 = myDenseLayer(n_hidden1, n_inputs)\n",
    "l2 = myDenseLayer(n_hidden2, n_hidden1)\n",
    "l3 = myDenseLayer(n_classes, n_hidden2)\n",
    "\n",
    "layers = [l1, l2, l3]\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(l1.wegt.shape, l1.bias.shape)\n",
    "print(l2.wegt.shape, l2.bias.shape)\n",
    "print(l3.wegt.shape, l3.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights are initialized to...\n",
    "l1.wegt = np.random.randn(n_hidden1, n_inputs)\n",
    "l2.wegt = np.random.randn(n_hidden2, n_hidden1)\n",
    "l3.wegt = np.random.randn(n_classes, n_hidden2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXCZlANPTa46"
   },
   "source": [
    "Define a Function for Splitting Dataset into mini-Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1_dZER6VTa49"
   },
   "outputs": [],
   "source": [
    "def create_mini_batches(X, y, batch_size=64):\n",
    "    mini_batches = []\n",
    "    n_minibatches = (X.shape[0] // batch_size)\n",
    "    n_variables = X.shape[1]\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    data = np.hstack((X,y))      # concatenate X and y with np.hstack\n",
    "    np.random.shuffle(data)      # then shuffle it\n",
    "    \n",
    "    for i in range(n_minibatches):\n",
    "        mini_batch = data[i*batch_size:(i+1)*batch_size,:]        # get a slice of mini-batch\n",
    "        X_mini, y_mini = np.split(mini_batch,[-y.shape[1]],axis=1)    # split mini-batch into X & y\n",
    "        mini_batches.append((X_mini, y_mini))\n",
    "    \n",
    "    if data.shape[0] % batch_size != 0:\n",
    "        mini_batch = data[n_minibatches*batch_size:,:]        # process the remaining data\n",
    "        X_mini, y_mini = np.split(mini_batch,[-y.shape[1]],axis=1)    # split mini-batch into X & y\n",
    "        mini_batches.append((X_mini, y_mini))\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  5]\n",
      " [18 19]\n",
      " [12 13]\n",
      " [ 8  9]]\n",
      "[[-12]\n",
      " [-19]\n",
      " [-16]\n",
      " [-14]] \n",
      "\n",
      "[[ 0  1]\n",
      " [ 6  7]\n",
      " [ 2  3]\n",
      " [14 15]]\n",
      "[[-10]\n",
      " [-13]\n",
      " [-11]\n",
      " [-17]] \n",
      "\n",
      "[[16 17]\n",
      " [10 11]]\n",
      "[[-18]\n",
      " [-15]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "#test code\n",
    "a = np.arange(20).reshape(10,2)\n",
    "b = -np.arange(10,20).reshape(10,1)\n",
    "c = create_mini_batches(a, b, 4)\n",
    "for mini_X, mini_y in c:\n",
    "    print(mini_X)\n",
    "    print(mini_y, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expected outpu:\n",
    "```\n",
    "[[ 4  5]          [[ 0  1]           [[16 17] \n",
    " [18 19]           [ 6  7]            [10 11]] \n",
    " [12 13]           [ 2  3]           [[-18] \n",
    " [ 8  9]]          [14 15]]           [-15]]  \n",
    "[[-12]            [[-10]             \n",
    " [-19]             [-13]             \n",
    " [-16]             [-11]             \n",
    " [-14]]            [-17]]              \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Various Optimizers\n",
    "\n",
    "Stochastic Gradient $$ g_t = \\nabla J(W_t,x^{(i)},y^{(i)}), \\;\\text{for mini-batch}\\; (i) \\to (i:i+n) $$\n",
    "\n",
    "SGD with momentum $$ \\Delta W(t) = \\gamma \\Delta W (t-1) + \\alpha \\cdot g_t $$\n",
    "AdaGrad $$ \\Delta W(t) = {\\eta {1 \\over \\sqrt{\\delta_t + \\epsilon}}} \\odot g_t, \\;\\text{where}\\; \\delta_t = \\delta_{t-1} + g_t^2 $$\n",
    "RMSProp $$ \\Delta W(t) = {\\eta {1 \\over \\sqrt{\\delta_t + \\epsilon}}} \\odot g_t, \\;\\text{where}\\; \\delta_t = \\beta \\delta_{t-1} + (1-\\beta) g_t^2 $$\n",
    "Adam $$ \\Delta W(t) = {\\eta {\\hat{m}_t \\over \\sqrt{\\hat{v}_t} + \\epsilon}} \\odot g_t, \\;\\text{where}\\; \\hat{m}_t = {m_t \\over {1 - \\beta_1^t}}, \\; \\hat{v}_t = {v_t \\over {1 - \\beta_2^t}}, $$\n",
    "$$ \\text{and}\\; m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t, \\; v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 $$\n",
    "\n",
    "\n",
    "In this experiment mini-batch gradient is used for all optimization methods unless mentioned otherwise.<br>\n",
    "Investigate and discuss the effect on convergence of each optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myOptParam:\n",
    "    def __init__(self, n_out, n_in):\n",
    "        # Previous delta values for momentum optimizer\n",
    "        self.W_dt = np.zeros((n_out, n_in))\n",
    "        self.B_dt = np.zeros(n_out)\n",
    "        # Variables for other optimizers\n",
    "        self.W_mt = np.zeros((n_out, n_in))\n",
    "        self.B_mt = np.zeros(n_out)\n",
    "        self.W_vt = np.zeros((n_out, n_in))\n",
    "        self.B_vt = np.zeros(n_out)\n",
    "\n",
    "def my_optimizer(lyr, opt, W_grad, B_grad, solver='sgd', learning_rate=0.01, iter=1):\n",
    "    epsilon = 1e-8  # arbitrary small number\n",
    "    alpha = eta = learning_rate\n",
    "\n",
    "    if iter==0:\n",
    "        print('iteration should start from 1.')\n",
    "\n",
    "    # optimizer routines\n",
    "    if  solver=='sgd':\n",
    "        W_dlt = alpha * W_grad    #learning rate * gradient\n",
    "        B_dlt = alpha * B_grad    #learning rate * gradient\n",
    "    elif solver=='momentum':\n",
    "        gamma = 0.9               # default setting\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        W_dlt = gamma*opt.W_dt + alpha*W_grad              # momentum for previous delta\n",
    "        B_dlt = gamma*opt.B_dt + alpha*B_grad              # same goes for bias\n",
    "        opt.W_dt = W_dlt          # keep data for later use\n",
    "        opt.B_dt = B_dlt          # for bias, too\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "    elif solver=='adagrad':\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        opt.W_vt = opt.W_vt + (W_grad*W_grad)           # accumulate delta square (2nd momentum)\n",
    "        opt.B_vt = opt.B_vt + (B_grad*B_grad)           # accumulater for bias term\n",
    "        W_dlt = (eta/np.sqrt(opt.W_vt+epsilon))*W_grad              # calculate new delta for weight\n",
    "        B_dlt = (eta/np.sqrt(opt.B_vt+epsilon))*B_grad              # and for bias\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "    elif solver=='rmsprop':\n",
    "        beta2 = 0.9               # default setting\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        opt.W_vt = beta2*opt.W_vt + (1-beta2)*(W_grad*W_grad)         # blending with second momentum\n",
    "        opt.B_vt = beta2*opt.B_vt + (1-beta2)*(B_grad*B_grad)           # also doging samething for bias\n",
    "        W_dlt = (eta/np.sqrt(opt.W_vt+epsilon))*W_grad              # calculate new delta for weight\n",
    "        B_dlt = (eta/np.sqrt(opt.B_vt+epsilon))*B_grad             # and for bias\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    elif solver=='adam':\n",
    "        beta1, beta2 = 0.9, 0.99  # default setting\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        opt.W_mt = beta1*opt.W_mt + (1-beta1)*W_grad           # blending with first momentum\n",
    "        opt.B_mt = beta1*opt.B_mt + (1-beta1)*B_grad           # first momentum for bias\n",
    "        opt.W_vt = beta2*opt.W_vt + (1-beta2)*(W_grad*W_grad)           # blending with second momentum\n",
    "        opt.B_vt = beta2*opt.B_vt + (1-beta2)*(B_grad*B_grad)           # second momentum for bias\n",
    "        W_mc = opt.W_mt / (1-np.power(beta1,iter))               # bias correction of first momentum for weight\n",
    "        B_mc = opt.B_mt / (1-np.power(beta1,iter))               # and for bias term\n",
    "        W_vc = opt.W_vt / (1-np.power(beta2,iter))               # bias correction of second momentum for weight\n",
    "        B_vc = opt.B_vt / (1-np.power(beta2,iter))               # and for bias term\n",
    "        W_dlt = (eta/(np.sqrt(W_vc)+epsilon))*W_mc              # calculate new delat for weight\n",
    "        B_dlt = (eta/(np.sqrt(B_vc)+epsilon))*B_mc              # and for bias\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    else:  \n",
    "        print('optimizer error')\n",
    "\n",
    "    # Adjust weight\n",
    "    lyr.wegt = lyr.wegt - W_dlt\n",
    "    lyr.bias = lyr.bias - B_dlt\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sgd:\n",
      "[ 7.67789007  8.16882972 10.34203348 -3.22934657]\n",
      "For momentum:\n",
      "[14.46528172 15.04341688 19.30016537 -4.77070266]\n",
      "For adagrad:\n",
      "[22.50872929 22.74302212 28.47667875 -7.62607443]\n",
      "For rmsprop:\n",
      "[ 30.69802889  30.60433129  37.72651766 -10.62235939]\n",
      "For adam:\n",
      "[29.41774022 19.27573813 23.68071186  1.52919472]\n",
      "test passed.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(101)\n",
    "#random initializing\n",
    "lyr = myDenseLayer(2,3)\n",
    "opt = myOptParam(2,3)\n",
    "\n",
    "lyr.wegt = np.random.randn(2,3)\n",
    "lyr.bias = np.random.randn(2)\n",
    "opt.W_dt = np.random.randn(2,3)\n",
    "opt.B_dt = np.random.randn(2)\n",
    "opt.W_mt = np.random.randn(2,3)\n",
    "opt.B_mt = np.random.randn(2)\n",
    "opt.W_vt = np.abs(np.random.randn(2,3))\n",
    "opt.B_vt = np.abs(np.random.randn(2))\n",
    "\n",
    "W_grad = np.random.randn(2,3)\n",
    "B_grad = np.random.randn(2)\n",
    "\n",
    "# optimizer settings are: 'sgd', 'momentum', 'adagrad', 'rmsprop', 'adam'\n",
    "opts = ['sgd', 'momentum', 'adagrad', 'rmsprop', 'adam']\n",
    "expt = [[ 7.67789007,  8.16882972, 10.34203348, -3.22934657],\n",
    "        [14.46528172,  15.04341688, 19.30016537, -4.77070266],\n",
    "        [22.50872929,  22.74302212, 28.47667875, -7.62607443],\n",
    "        [30.69802889,  30.60433129, 37.72651766, -10.62235939],\n",
    "        [29.41774022,  19.27573813, 23.68071186,  1.52919472]]\n",
    "test_passed = True\n",
    "for i, sol in enumerate(opts):\n",
    "    my_optimizer(lyr, opt, W_grad, B_grad, sol, 10, 3)\n",
    "    print('For '+sol+':')\n",
    "    res = np.concatenate((lyr.wegt[0], lyr.bias[0:1]), axis=0)\n",
    "    print(res)\n",
    "    if not np.allclose(res, expt[i]): \n",
    "        print(sol+' failed.')\n",
    "        test_passed = False\n",
    "if test_passed: print('test passed.')\n",
    "else: print('test failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Outputs**\n",
    "\n",
    "For SGD:\n",
    "```\n",
    "[ 7.67789007  8.16882972 10.34203348 -3.22934657]\n",
    "```\n",
    "For Momentum:\n",
    "```\n",
    "[14.46528172 15.04341688 19.30016537 -4.77070266]\n",
    "```\n",
    "Fpr Adagrad:\n",
    "```\n",
    "[22.50872929 22.74302212 28.47667875 -7.62607443]\n",
    "```\n",
    "For RMSProp:\n",
    "```\n",
    "[ 30.69802889  30.60433129  37.72651766 -10.62235939]\n",
    "```\n",
    "For Adam:\n",
    "```\n",
    "[29.41774022 19.27573813 23.68071186  1.52919472]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Optimizer Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1 = myOptParam(n_hidden1, n_inputs)\n",
    "o2 = myOptParam(n_hidden2, n_hidden1)\n",
    "o3 = myOptParam(n_classes, n_hidden2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Simple Neural Network Model (3 layer model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55777,
     "status": "ok",
     "timestamp": 1649259680196,
     "user": {
      "displayName": "Seong-Won Lee",
      "userId": "14858304546124675216"
     },
     "user_tz": -540
    },
    "id": "qODinrZlTa5C",
    "outputId": "8237949a-964a-49cd-f3e9-fb65759245e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  100,  loss: 0.00000031\n",
      "Epoch:  200,  loss: 0.00000000\n",
      "Epoch:  300,  loss: 0.00000000\n",
      "Epoch:  400,  loss: 0.00000000\n",
      "Epoch:  500,  loss: 0.00000000\n",
      "Epoch:  600,  loss: 0.00000000\n",
      "Epoch:  700,  loss: 0.00000000\n",
      "Epoch:  800,  loss: 0.00000000\n",
      "Epoch:  900,  loss: 0.00000000\n",
      "Epoch: 1000,  loss: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "# optimizer settings are: 'sgd', 'momentum', 'adagrad', 'rmsprop', 'adam'\n",
    "# alpha is learning rate\n",
    "optimizer ='adam'\n",
    "alpha = 0.01\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    batches = create_mini_batches(X_train, y_train, batch_size=64)\n",
    "    for one_batch in batches:\n",
    "        X_mini, y_mini = one_batch\n",
    "        batch_len = X_mini.shape[0]  # last batch might have different length\n",
    "\n",
    "        # Forward Path\n",
    "        a_1, a_2, a_3 = my_forward(layers, X_mini)\n",
    "        \n",
    "        # Backward Path\n",
    "        d_1, d_2, d_3 = my_backward(layers, a_1, a_2, a_3, X_mini, y_mini)\n",
    "\n",
    "        dw_1, db_1 = d_1\n",
    "        dw_2, db_2 = d_2\n",
    "        dw_3, db_3 = d_3\n",
    "        \n",
    "        # Update weights and biases\n",
    "        my_optimizer(l1, o1, dw_1, db_1, solver=optimizer, learning_rate=alpha, iter=epoch+1)\n",
    "        my_optimizer(l2, o2, dw_2, db_2, solver=optimizer, learning_rate=alpha, iter=epoch+1)\n",
    "        my_optimizer(l3, o3, dw_3, db_3, solver=optimizer, learning_rate=alpha, iter=epoch+1)\n",
    "\n",
    "    if ((epoch+1)%100==0):\n",
    "        loss_J = my_loss(layers, X_train, y_train)\n",
    "        print('Epoch: %4d,  loss: %10.8f' % (epoch+1, loss_J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1649259686353,
     "user": {
      "displayName": "Seong-Won Lee",
      "userId": "14858304546124675216"
     },
     "user_tz": -540
    },
    "id": "xMvLn6SJTa5D",
    "outputId": "229cafc3-c9c5-4dd2-f7bc-d4cdf90c4d46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722222222222222"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = my_predict(layers, X_test)\n",
    "\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WieUxPz9Ta5F"
   },
   "source": [
    "Neural Network from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3086,
     "status": "ok",
     "timestamp": 1649259694763,
     "user": {
      "displayName": "Seong-Won Lee",
      "userId": "14858304546124675216"
     },
     "user_tz": -540
    },
    "id": "w8AcitiaTa5G",
    "outputId": "09455c7b-a58f-4492-b5cb-431eab436f6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(80, 70, ), activation='logistic', solver='sgd', \\\n",
    "                    alpha=0.01, learning_rate_init=0.01, max_iter=1000)\n",
    "\n",
    "# Training/Fitting the Model\n",
    "mlp.fit(X_train, y_train_num)\n",
    "\n",
    "# Making Predictions\n",
    "s_pred = mlp.predict(X_test)\n",
    "accuracy_score(s_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmZQrDH9n0PK"
   },
   "source": [
    "### Test Model with a random sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My prediction is 3\n",
      "sk prediction is 3\n",
      "Actual number is 3\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(X_test.shape[0])\n",
    "dimage = X_test_org[idx].reshape((8,8))\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.gray()\n",
    "plt.matshow(dimage, fignum=1)\n",
    "plt.show()\n",
    "\n",
    "X_input = np.expand_dims(X_test[idx], 0)\n",
    "\n",
    "y_pred = my_predict(layers, X_input)\n",
    "\n",
    "s_pred = mlp.predict(X_input)\n",
    "\n",
    "print('My prediction is ' + str(y_pred[0]))\n",
    "print('sk prediction is ' + str(s_pred[0]))\n",
    "print('Actual number is ' + str(y_test[idx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 2024 SW Lee"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_L06_01_MNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "71930d9c743a2c2f7d41567bb1e631f1d30be1b0f7ff3429fb86acce8edbed56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
